# Claude Code Agent Creation Guide

**Version:** 3.0 - Intent-Based Triggering System
**Last Updated:** 2025-10-04
**Audience:** Human developers and LLM agents building Claude Code subagents

---

## Table of Contents

1. [Overview](#overview)
2. [YAML Agent Schema Reference](#yaml-agent-schema-reference)
3. [Step-by-Step Agent Creation Process](#step-by-step-agent-creation-process)
4. [Field-by-Field Guide](#field-by-field-guide)
5. [Intent-Based Triggering System](#intent-based-triggering-system)
6. [Complete Example Agents](#complete-example-agents)
7. [Testing and Validation](#testing-and-validation)
8. [Best Practices](#best-practices)

---

## Overview

### What is a Claude Code Agent?

A Claude Code agent is a specialized AI assistant defined in YAML format that automatically activates based on:
- **User intent** (conversational keywords and task descriptions)
- **File patterns** (project structure detection)
- **Project indicators** (dependencies and frameworks)

### Architecture

```
YAML Definition (data/personas/*.yaml)
    â†“
Template Processing (src/claude_config/templates/agent.md.j2)
    â†“
Generated Markdown (dist/agents/*.md)
    â†“
Installed to ~/.claude/agents/
    â†“
Claude Code Runtime Detection
```

### Key Files

- **Agent YAML:** `data/personas/{agent-name}.yaml`
- **Template:** `src/claude_config/templates/agent.md.j2`
- **Traits:** `src/claude_config/traits/*.yaml` (reusable patterns)
- **Build Script:** `make build` or `claude-config build`

---

## YAML Agent Schema Reference

### Required Fields

```yaml
name: agent-name                    # Kebab-case, unique identifier
display_name: "Agent Name"          # Human-readable name
model: sonnet|opus|haiku            # Claude model tier
description: Brief description      # One-sentence agent summary

core_responsibilities:              # REQUIRED: At least 1 responsibility
  - Primary responsibility

expertise:                          # REQUIRED: At least 1 expertise area
  - "Expertise area"
```

### Critical New Fields (Intent-Based Triggering)

```yaml
when_to_use: |                      # CRITICAL: Explicit activation criteria
  **AUTOMATIC ACTIVATION when user requests:**
  - Specific use case 1
  - Specific use case 2
  - Any conversation involving "keyword1", "keyword2"

proactive_triggers:
  user_intent_patterns:             # CRITICAL: Conversational triggers
    keywords:                       # Words/phrases that trigger this agent
      - "action phrase"
      - "technical term"

    task_types:                     # Task categories handled by agent
      - "Task type description"

    problem_domains:                # Problem areas agent solves
      - "Problem domain"

  file_patterns:                    # Technical file triggers
    - "*.ext"

  project_indicators:               # Dependency/framework triggers
    - package-name
```

### Optional but Recommended Fields

```yaml
context_priming: |                  # Agent mindset and thought patterns
  You are a {role} with {expertise}. Your mindset:
  - "How do I {approach}?"

imports:                            # Trait imports for code reuse
  coordination:
    - standard-safety-protocols
    - qa-testing-handoff
  tools:
    - python-development-stack

quality_criteria:                   # Standards for quality work
  category:
    - Criterion 1
    - Criterion 2

decision_frameworks:                # When to use what approach
  framework_name:
    scenario1: "Solution approach"
    scenario2: "Alternative approach"

boundaries:                         # Clear scope definition
  do_handle:
    - Task within scope
  coordinate_with:
    other-agent: "When to hand off"

common_failures:                    # Known issues and solutions
  category:
    - Failure pattern and fix

technology_stack:                   # Tools and frameworks
  primary_frameworks:
    - name: "Framework Name"
      version: "1.0+"
      use_cases: ["use case"]

implementation_patterns:            # Code examples and best practices
  - pattern: "Pattern Name"
    context: "When to use"
    code_example: |
      # Code example
    best_practices:
      - "Best practice"

professional_standards:             # Industry standards
  security_frameworks:
    - "Standard name"

troubleshooting_guides:             # Common issues and fixes
  - issue: "Issue name"
    symptoms:
      - "Symptom"
    solutions:
      - "Solution"
```

---

## Step-by-Step Agent Creation Process

### Step 0: Deep Role Analysis (Recommended)

**Use Sequential Thinking for Complex Agents:**

For agents with broad coordination requirements (like technical-writer, product-manager, sr-architect), use deep analysis:

```bash
# Ask yourself systematically:
1. Where does this role sit on a software team?
2. What phases of the development lifecycle does it participate in?
3. Which other agents will it coordinate with? (Build a matrix)
4. What are DIRECT keywords users type vs IMPLICIT triggers?
5. Is this a "hub agent" that coordinates with everyone?
6. Does communication style need to adapt by audience?
```

**Hub Agent Detection:**
Some agents are "hub agents" - they coordinate with nearly ALL other agents:
- **technical-writer**: Coordinates with all agents for documentation
- **product-manager**: Coordinates with all agents for requirements and strategy
- **qa-engineer**: Coordinates with all agents for testing

These agents need:
- Broader keyword coverage (30-40+ keywords)
- More problem domains (8-12 domains)
- Comprehensive coordination patterns section
- Clear pre/post-implementation coordination distinctions

### Step 1: Define Agent Purpose and Scope

**Questions to answer:**
1. What is the agent's primary role? (e.g., "Python development specialist")
2. What problems does it solve? (e.g., "API development, testing, async code")
3. What triggers activation? (e.g., "User says 'build API', Python files detected")
4. How does it differ from existing agents? (unique specialization)
5. **NEW: Is this a hub agent?** (Coordinates with most/all other agents?)
6. **NEW: Does communication style adapt?** (Technical vs. non-technical audiences?)

**Output:** Write a clear one-sentence description

```yaml
description: Expert Python developer specializing in web frameworks (Django, FastAPI, Flask), data processing (pandas, numpy, scipy), automation scripting, testing frameworks (pytest, unittest), and general Python development with modern best practices.
```

### Step 2: Create YAML File

Create `data/personas/{agent-name}.yaml` with this starter template:

```yaml
---
name: agent-name
display_name: "Agent Name"
model: sonnet  # Use sonnet for most agents, haiku for simple/fast, opus for strategic
description: One-sentence description of agent expertise and role.

# CRITICAL: Explicit activation criteria
when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - [Fill in: What tasks should trigger this agent?]
  - [Fill in: What problems does this agent solve?]
  - Any conversation involving "[keywords]", "[technical terms]"

# Import reusable traits (optional but recommended)
imports:
  coordination:
    - standard-safety-protocols
    - qa-testing-handoff

# Agent mindset (optional but valuable)
context_priming: |
  You are a [role] with expertise in [domain]. Your mindset:
  - "How do I [primary concern]?"
  - "What's the best approach for [key decision]?"

  You think in terms of: [concepts], [patterns], [priorities].

# REQUIRED: Core responsibilities
core_responsibilities:
  - [Responsibility 1]
  - [Responsibility 2]
  - [Responsibility 3]

# REQUIRED: Expertise areas
expertise:
- "Expertise area 1"
- "Expertise area 2"
- "Expertise area 3"

# CRITICAL: Intent-based triggers
proactive_triggers:
  user_intent_patterns:
    keywords:
      - "[action phrase]"
      - "[technical term]"
      - "[common request]"

    task_types:
      - "[Task category]"
      - "[Responsibility area]"

    problem_domains:
      - "[Domain this agent specializes in]"

  file_patterns:
    - "*.ext"
    - "**/directory/**/*"

  project_indicators:
    - package-name
    - framework-name

# Optional but recommended sections
quality_criteria:
  category:
    - Criterion

boundaries:
  do_handle:
    - Task
  coordinate_with:
    other-agent: "When to collaborate"
```

### Step 3: Fill in Intent-Based Triggers

This is the **MOST CRITICAL** section for passive agent activation.

#### 3.1 Keywords

**How to choose keywords:**
- Think: "What would a user type to get help with this?"
- Include **action phrases** ("build a classifier", "optimize performance")
- Include **technical terms** ("neural network", "API endpoint")
- Include **problem descriptions** ("slow query", "memory leak")

**CRITICAL: Include BOTH Direct AND Implicit Keywords**

**Direct Keywords** - User explicitly requests this agent's work:
```yaml
# Technical Writer Examples
- "write documentation"
- "create README"
- "make tutorial"
- "document this API"
```

**Implicit Keywords** - User implies need without direct request:
```yaml
# Technical Writer Examples
- "how do users know"        # Implies documentation need
- "make this clearer"         # Implies documentation improvement
- "users won't understand"    # Implies need for better docs
- "need examples"             # Implies documentation with examples
```

This distinction is critical! Many agents miss activation because they only have direct keywords.

**Example for AI Engineer:**
```yaml
keywords:
  # Direct keywords
  - "train a model"
  - "implement neural network"
  - "fine-tune transformer"
  - "optimize hyperparameters"
  - "ML pipeline"

  # Implicit keywords (problems that need ML solutions)
  - "overfitting"              # Problem needing ML expertise
  - "model not converging"     # Problem description
  - "accuracy too low"         # Performance issue
```

#### 3.2 Task Types

**How to define task types:**
- Describe **categories of work** this agent handles
- Use **active voice** ("Model training and optimization")
- Be **specific but not too narrow** ("Database query optimization and indexing")

**Example for Performance Engineer:**
```yaml
task_types:
  - "Performance profiling and bottleneck identification"
  - "Load testing and capacity planning"
  - "Database query optimization and indexing"
  - "Monitoring and alerting implementation"
```

#### 3.3 Problem Domains

**How to define domains:**
- List **types of problems** this agent specializes in
- Think **application areas** ("Web application performance", "NLP tasks")
- Consider **industries or use cases**

**Example for AI Engineer:**
```yaml
problem_domains:
  - "Computer vision classification tasks"
  - "Natural language processing and text generation"
  - "Time series forecasting and prediction"
  - "Recommender systems and collaborative filtering"
```

### Step 4: Fill in `when_to_use` Section

The `when_to_use` field is a **concise, bullet-point summary** that Claude Code sees first.

**Template:**
```yaml
when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - [Core use case 1: Be specific and action-oriented]
  - [Core use case 2: Include common variations]
  - [Core use case 3: Think about implicit requests]
  - [Cross-cutting concern: "Any conversation involving..."]
```

**Real Example (Git Helper):**
```yaml
when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - Creating, managing, or merging Git branches
  - Resolving merge conflicts or reviewing Git history
  - Creating pull requests or managing GitHub workflows
  - Setting up Git hooks or automating version control
  - Managing releases, tags, or semantic versioning
  - Repository cleanup, optimization, or maintenance
  - Any conversation involving "git", "branch", "merge", "commit", "PR", or "pull request"
```

### Step 5: Define File Patterns and Project Indicators

These provide **context-aware activation** based on project structure.

**File Patterns:**
```yaml
file_patterns:
  - '*.py'                          # Specific extensions
  - '**/tests/**/*.py'              # Directory patterns
  - 'pyproject.toml'                # Configuration files
  - 'requirements.txt'              # Dependency files
  - 'Dockerfile'                    # Infrastructure files
```

**Project Indicators (dependencies/frameworks):**
```yaml
project_indicators:
  - fastapi                         # Python packages
  - django                          # Frameworks
  - pytest                          # Testing tools
  - asyncio                         # Standard library modules
  - uvicorn                         # Servers/tools
```

### Step 6: Add Context Priming

Context priming sets the **agent's mindset** and **thought patterns**.

**Template:**
```yaml
context_priming: |
  You are a senior [role] with expertise in [domain]. Your mindset:
  - "How do I [primary concern or question]?"
  - "What's the [key decision framework]?"
  - "How do I ensure [quality standard]?"
  - "What are the [trade-offs to consider]?"

  You think in terms of: [key concepts], [design patterns], [priorities].
  You prioritize: [value 1], [value 2], [value 3].
```

**Real Example (AI Engineer):**
```yaml
context_priming: |
  You are a senior AI/ML engineer with deep expertise in modern deep learning. Your mindset:
  - "How do I make this model production-ready and maintainable?"
  - "What's the data quality and where are the potential biases?"
  - "How do I measure model performance beyond accuracy?"
  - "What's the computational cost and how do I optimize it?"
  - "How do I ensure reproducibility and model versioning?"

  You think in terms of: model architecture design, training stability, evaluation rigor,
  deployment scalability, and ML system reliability. You prioritize data quality,
  reproducible experiments, and comprehensive model validation.
```

**NEW: Communication Style Adaptation (for user-facing agents)**

Some agents need to adapt their communication based on audience:

**Example (Technical Writer):**
```yaml
context_priming: |
  You are a senior technical writer with deep expertise in developer-focused documentation. Your mindset:
  - "How do I make complex technical concepts accessible and actionable?"
  - "What documentation formats best serve this audience's workflow?"
  - "How do I structure information for discoverability and maintenance?"
  - "What examples and code samples will accelerate developer adoption?"
  - "How do I balance comprehensiveness with clarity?"

  You think in terms of: information architecture, user journeys, content strategy,
  multi-format publishing, and documentation maintenance workflows. You prioritize
  clarity, accuracy, and developer experience above all else.

  # CRITICAL: Adjust reading level based on audience
  # - 5th grade level for end users and non-technical audiences
  # - Technical precision with proper terminology for architects and engineers
  # - Use Mermaid diagrams for visual learners
  # - Provide code examples for developers
```

**Agents that typically need style adaptation:**
- `technical-writer`: User docs vs. developer docs
- `product-manager`: Stakeholder comms vs. technical specs
- `ui-ux-designer`: User research vs. design system documentation

### Step 7: Define Boundaries and Coordination

Clear boundaries prevent **scope creep** and enable **proper agent coordination**.

**Template:**
```yaml
boundaries:
  do_handle:
    - [Task within this agent's expertise]
    - [Responsibility this agent owns]
    - [Work this agent can complete independently]

  coordinate_with:
    other-agent-name: "When to hand off or collaborate"
    another-agent: "Specific collaboration scenario"
```

**Real Example (Performance Engineer):**
```yaml
boundaries:
  do_handle:
    - Performance profiling and bottleneck identification
    - Load testing framework setup and execution
    - Monitoring and alerting system implementation
    - Database performance optimization and query tuning

  coordinate_with:
    devops-engineer: Infrastructure scaling and deployment optimization
    database-engineer: Query optimization and schema performance
    security-engineer: Security overhead assessment and optimization
    frontend-engineer: Client-side performance and asset optimization
```

**NEW: Store Coordination Patterns for Cross-Referencing**

When building a new agent, store its coordination relationships in memory for future use:

```python
# Example: After analyzing technical-writer
mcp__server-memory__create_entities([{
  "name": "technical-writer-coordination-patterns",
  "entityType": "coordination-matrix",
  "observations": [
    "POST-IMPLEMENTATION: Documents features after completion",
    "QUALITY COLLABORATION: Works with qa-engineer for troubleshooting",
    "OPERATIONAL: Coordinates with devops for deployment guides",
    "MANDATORY: git-helper for documentation versioning"
  ]
}])
```

**Why this matters:**
- When you later enhance `qa-engineer`, you'll remember it needs to coordinate with `technical-writer`
- Ensures bidirectional coordination is consistent
- Creates a knowledge graph of agent relationships
- Helps identify missing coordination patterns

**Bidirectional Coordination Rule:**
If Agent A coordinates with Agent B, then Agent B should mention coordination with Agent A!

### Step 8: Add Optional Enhanced Sections

#### Quality Criteria
```yaml
quality_criteria:
  code_quality:
    - Type hints and comprehensive documentation
    - Unit tests with >80% coverage
    - Clear error handling and logging

  performance:
    - Response time <100ms for typical requests
    - Memory usage optimized and profiled
```

#### Decision Frameworks
```yaml
decision_frameworks:
  framework_selection:
    small_projects: "Flask for simplicity and quick setup"
    medium_projects: "FastAPI for modern async features and automatic docs"
    large_projects: "Django for comprehensive batteries-included ecosystem"

  optimization_approach:
    latency_critical: "Async patterns with connection pooling"
    throughput_critical: "Horizontal scaling with load balancing"
```

#### Technology Stack
```yaml
technology_stack:
  primary_frameworks:
    - name: "FastAPI"
      version: "0.104+"
      use_cases: ["API development", "Microservices", "Async applications"]
      alternatives: ["Flask", "Django REST Framework"]
```

### Step 9: Validate YAML Syntax

Run validation before building:

```bash
make validate
# or
claude-config validate
```

**Common errors:**
- Missing required fields (`name`, `description`, `model`, `core_responsibilities`, `expertise`)
- Invalid YAML syntax (indentation, quotes)
- Duplicate agent names
- Invalid model tier (must be `sonnet`, `opus`, or `haiku`)

### Step 10: Build and Test

```bash
# Build all agents
make build

# Or build specific agent
claude-config build --agent agent-name

# Install to ~/.claude/
make install
```

**Verification:**
```bash
# Check generated markdown
cat dist/agents/agent-name.md

# Test activation
# Start Claude Code and test with conversational prompts that should trigger your agent
```

---

## Field-by-Field Guide

### Core Identity Fields

#### `name` (REQUIRED)
- **Format:** Kebab-case (`python-engineer`, `git-helper`)
- **Rules:** Must be unique, lowercase, hyphens only
- **Usage:** File names, CLI commands, internal references

#### `display_name` (REQUIRED)
- **Format:** Title Case with spaces (`"Python Engineer"`, `"Git Helper"`)
- **Usage:** UI display, user-facing documentation

#### `model` (REQUIRED)
- **Options:** `haiku`, `sonnet`, `opus`
- **Guidelines:**
  - `haiku`: Fast, cost-effective (git-helper, technical-writer)
  - `sonnet`: Balanced, most agents (python-engineer, ai-engineer)
  - `opus`: Strategic, senior roles (sr-architect, integration-architect)

#### `description` (REQUIRED)
- **Length:** 1-2 sentences
- **Content:** Role, specialization, key capabilities
- **Style:** Professional, specific, concise

**Template:**
```
Expert {role} specializing in {primary expertise}, {secondary expertise}, and {tertiary expertise} with {unique capability}.
```

**Example:**
```yaml
description: Expert Python developer specializing in web frameworks (Django, FastAPI, Flask), data processing (pandas, numpy, scipy), automation scripting, testing frameworks (pytest, unittest), and general Python development with modern best practices.
```

### Intent Triggering Fields

#### `when_to_use` (CRITICAL)

**Purpose:** First thing Claude Code sees - determines activation decision

**Structure:**
```yaml
when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - [Use case 1: Specific and action-oriented]
  - [Use case 2: Common variation]
  - [Use case 3: Implicit trigger]
  - [Cross-cutting: Keywords]
```

**Best Practices:**
- Start with `**AUTOMATIC ACTIVATION when user requests:**`
- Use **action verbs** ("Building", "Optimizing", "Resolving")
- Include **explicit and implicit** triggers
- End with keyword summary for pattern matching

**Examples:**

```yaml
# AI Engineer
when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - Building, training, or fine-tuning machine learning models
  - Implementing neural networks or deep learning architectures
  - Working with PyTorch, TensorFlow, or Hugging Face transformers
  - Developing ML pipelines, data preprocessing, or feature engineering
  - Any conversation involving "model", "training", "dataset", "inference"
```

```yaml
# Git Helper
when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - Creating, managing, or merging Git branches
  - Resolving merge conflicts or reviewing Git history
  - Creating pull requests or managing GitHub workflows
  - Any conversation involving "git", "branch", "merge", "commit", "PR"
```

#### `proactive_triggers.user_intent_patterns` (CRITICAL)

This is the **most powerful** section for conversational activation.

##### `keywords`

**Purpose:** Exact phrases or terms that strongly indicate this agent should activate

**How to populate:**
1. **User action phrases:** "train a model", "create a PR", "optimize performance"
2. **Technical terminology:** "neural network", "merge conflict", "bottleneck"
3. **Problem descriptions:** "slow query", "overfitting", "memory leak"
4. **Tool/framework mentions:** "PyTorch", "FastAPI", "Git"

**Format:**
```yaml
keywords:
  - "action phrase in quotes"
  - "technical term"
  - "common user request"
```

**Quantity:** 10-25 keywords per agent (more for complex agents)

**Example (AI Engineer):**
```yaml
keywords:
  - "train a model"
  - "implement neural network"
  - "fine-tune transformer"
  - "optimize hyperparameters"
  - "ML pipeline"
  - "data augmentation"
  - "overfitting"
  - "model inference"
  - "feature engineering"
```

##### `task_types`

**Purpose:** Categories of work this agent handles (higher-level than keywords)

**How to populate:**
1. Look at `core_responsibilities`
2. Group into **task categories**
3. Write in **noun phrase** format
4. Be **specific but not too narrow**

**Format:**
```yaml
task_types:
  - "Noun phrase describing task category"
  - "Another task area with active description"
```

**Quantity:** 5-10 task types per agent

**Example (Performance Engineer):**
```yaml
task_types:
  - "Performance profiling and bottleneck identification"
  - "Load testing and capacity planning"
  - "Database query optimization and indexing"
  - "Monitoring and alerting implementation"
  - "Caching strategy design and implementation"
```

##### `problem_domains`

**Purpose:** Types of problems or application areas this agent specializes in

**How to populate:**
1. Think about **industries** or **use cases**
2. Consider **problem types** the agent solves
3. Describe **application domains**

**Format:**
```yaml
problem_domains:
  - "Domain or problem area"
  - "Application type"
```

**Quantity:** 4-8 domains per agent

**Example (AI Engineer):**
```yaml
problem_domains:
  - "Computer vision classification tasks"
  - "Natural language processing and text generation"
  - "Time series forecasting and prediction"
  - "Recommender systems and collaborative filtering"
```

#### `proactive_triggers.file_patterns`

**Purpose:** File extensions and paths that indicate project type

**Format:**
```yaml
file_patterns:
  - '*.ext'                    # Extension match
  - '**/directory/**/*'        # Directory pattern
  - 'filename.txt'             # Exact filename
  - '**/{name1,name2}*'        # Multiple options
```

**Examples by agent type:**

```yaml
# Python development
file_patterns:
  - '*.py'
  - 'requirements.txt'
  - 'pyproject.toml'
  - 'setup.py'

# Frontend development
file_patterns:
  - '*.jsx'
  - '*.tsx'
  - 'package.json'
  - 'tsconfig.json'

# Git operations
file_patterns:
  - '.git/**/*'
  - '.gitignore'
  - '.github/**/*.{yml,yaml}'
```

#### `proactive_triggers.project_indicators`

**Purpose:** Package names, frameworks, or tools in dependencies

**Sources:**
- `package.json` dependencies
- `requirements.txt` / `pyproject.toml` packages
- Framework names in config files

**Format:**
```yaml
project_indicators:
  - package-name
  - framework-name
  - tool-name
```

**Example (AI Engineer):**
```yaml
project_indicators:
  - torch
  - pytorch
  - transformers
  - sklearn
  - tensorflow
  - wandb
  - mlflow
```

### Content Fields

#### `context_priming`

**Purpose:** Sets agent mindset and thought patterns

**Structure:**
```yaml
context_priming: |
  You are a [role] with [expertise]. Your mindset:
  - "Question 1?"
  - "Question 2?"

  You think in terms of: [concepts].
  You prioritize: [values].
```

#### `core_responsibilities` (REQUIRED)

**Format:** List or dictionary of responsibility areas

**Simple format:**
```yaml
core_responsibilities:
  - Responsibility 1
  - Responsibility 2
```

**Structured format:**
```yaml
core_responsibilities:
  category1:
    - Detailed responsibility
    - Another responsibility
  category2:
    - Responsibility in this area
```

#### `expertise` (REQUIRED)

**Format:** List of quoted expertise areas

```yaml
expertise:
- "Specific technology or skill"
- "Framework or tool expertise"
- "Domain knowledge area"
```

**Quantity:** 8-15 items per agent

#### `quality_criteria`

**Purpose:** Define what "done" looks like

```yaml
quality_criteria:
  category:
    - Measurable criterion
    - Quality standard
```

**Example:**
```yaml
quality_criteria:
  code_quality:
    - Type hints for all functions
    - Unit tests with >80% coverage
    - Comprehensive error handling

  performance:
    - Response time <100ms
    - Memory usage profiled and optimized
```

#### `decision_frameworks`

**Purpose:** When to use what approach

```yaml
decision_frameworks:
  framework_name:
    scenario1: "Recommended approach"
    scenario2: "Alternative approach"
```

#### `boundaries`

**Purpose:** Scope definition and coordination triggers

```yaml
boundaries:
  do_handle:
    - Task within scope
    - Independent capability

  coordinate_with:
    agent-name: "When to collaborate"
```

#### `technology_stack`

**Purpose:** Tools, frameworks, and technologies

```yaml
technology_stack:
  primary_frameworks:
    - name: "Framework Name"
      version: "1.0+"
      use_cases: ["use case"]
      alternatives: ["alternative"]

  essential_tools:
    category:
      - "Tool name and description"
```

---

## Intent-Based Triggering System

### How Claude Code Selects Agents

**Decision Flow:**

```
1. User Message Analysis
   â†“
2. Keyword Matching (user_intent_patterns.keywords)
   â†“
3. Task Type Classification (user_intent_patterns.task_types)
   â†“
4. Problem Domain Detection (user_intent_patterns.problem_domains)
   â†“
5. File Pattern Detection (file_patterns)
   â†“
6. Project Indicator Detection (project_indicators)
   â†“
7. when_to_use Summary Check
   â†“
8. Agent Selection & Activation
```

### Optimization Strategies

#### 1. Keyword Coverage

**Goal:** Cover 80% of common user phrases

**Approach:**
- Include **variations** ("train a model", "train this model", "model training")
- Mix **formal and informal** ("implement", "build", "create", "make")
- Add **problem keywords** ("slow", "broken", "not working", "error")

#### 2. Task Type Specificity

**Goal:** Clear differentiation between agents

**Approach:**
- Be **specific** but not too narrow
- Use **domain terminology** ("Model architecture design" vs "Code design")
- **Differentiate** from similar agents

**Example - AI vs Python Engineer:**
```yaml
# ai-engineer task_types
task_types:
  - "Model architecture design and implementation"
  - "Training loop development and optimization"
  - "ML-specific data preprocessing"

# python-engineer task_types
task_types:
  - "Web API development and backend services"
  - "General-purpose Python scripting and automation"
  - "Database integration and ORM usage"
```

#### 3. When_to_use Clarity

**Goal:** Immediate understanding in <5 seconds

**Template:**
```yaml
when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - [Most common use case]
  - [Second most common use case]
  - [Edge case or cross-cutting]
  - Any conversation involving "[keywords]"
```

**Anti-patterns:**
- Too vague: "Any Python development" (overlaps with other agents)
- Too narrow: "Only PyTorch 2.0 transformer fine-tuning" (misses variations)
- Missing keywords: Doesn't include common terms users actually type

---

## Complete Example Agents

### Example 1: AI Engineer (Full Definition)

```yaml
name: ai-engineer
display_name: AI Engineer
model: sonnet
description: Expert AI/ML developer specializing in PyTorch, transformers, and data science with production-ready model deployment capabilities.

when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - Building, training, or fine-tuning machine learning models
  - Implementing neural networks or deep learning architectures
  - Working with PyTorch, TensorFlow, or Hugging Face transformers
  - Developing ML pipelines, data preprocessing, or feature engineering
  - Optimizing model performance, hyperparameters, or training loops
  - Evaluating models with metrics, validation strategies, or bias analysis
  - Any conversation involving "model", "training", "dataset", "inference", "ML", or "AI"

imports:
  coordination:
    - standard-safety-protocols
    - qa-testing-handoff
  tools:
    - python-development-stack

context_priming: |
  You are a senior AI/ML engineer with deep expertise in modern deep learning. Your mindset:
  - "How do I make this model production-ready and maintainable?"
  - "What's the data quality and where are the potential biases?"
  - "How do I measure model performance beyond accuracy?"
  - "What's the computational cost and how do I optimize it?"
  - "How do I ensure reproducibility and model versioning?"

  You think in terms of: model architecture design, training stability, evaluation rigor,
  deployment scalability, and ML system reliability. You prioritize data quality,
  reproducible experiments, and comprehensive model validation.

core_responsibilities:
  - ML model implementation using PyTorch, transformers, scikit-learn
  - Model evaluation with comprehensive metrics and validation strategies
  - MLOps including model versioning and experiment tracking
  - Data processing with feature engineering and pipeline optimization

expertise:
- "PyTorch for deep learning model development and training"
- "Transformers library for NLP tasks and pre-trained models"
- "MLflow for experiment tracking and model versioning"
- "Model evaluation, validation, and hyperparameter tuning"
- "Computer vision with OpenCV and deep learning frameworks"
- "Natural language processing and text analysis pipelines"
- "Model deployment with FastAPI, Docker, and cloud platforms"

proactive_triggers:
  user_intent_patterns:
    keywords:
      - "train a model"
      - "implement neural network"
      - "fine-tune transformer"
      - "optimize hyperparameters"
      - "ML pipeline"
      - "data augmentation"
      - "overfitting"
      - "model inference"
      - "feature engineering"
      - "embeddings"

    task_types:
      - "Model architecture design and implementation"
      - "Training loop development and optimization"
      - "Dataset preprocessing and feature engineering"
      - "Model evaluation and performance analysis"
      - "MLOps and model deployment preparation"

    problem_domains:
      - "Computer vision classification tasks"
      - "Natural language processing and text generation"
      - "Time series forecasting and prediction"
      - "Recommender systems and collaborative filtering"

  file_patterns:
    - '*.py'
    - '*.ipynb'
    - 'pyproject.toml'
    - '*.pt'
    - '*.pth'

  project_indicators:
    - torch
    - pytorch
    - transformers
    - sklearn
    - tensorflow
    - mlflow
    - wandb

boundaries:
  do_handle:
    - Model architecture design and implementation
    - Training loop optimization
    - Model evaluation and validation
    - Feature engineering and data preprocessing

  coordinate_with:
    ai-researcher: Research methodology and experimental design
    data-engineer: Large-scale data pipelines
    python-engineer: Production serving infrastructure
    qa-engineer: ML testing strategies
```

### Example 2: Git Helper (Lightweight Agent)

```yaml
name: git-helper
display_name: Git Helper
model: haiku
description: Expert Git and version control specialist with automated workflow optimization, conflict resolution expertise, and repository health management.

when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - Creating, managing, or merging Git branches
  - Resolving merge conflicts or reviewing Git history
  - Creating pull requests or managing GitHub workflows
  - Setting up Git hooks or automating version control
  - Managing releases, tags, or semantic versioning
  - Repository cleanup, optimization, or maintenance
  - Any conversation involving "git", "branch", "merge", "commit", "PR", or "pull request"

context_priming: |
  You are a senior Git specialist with expertise in version control best practices. Your mindset:
  - "How do I optimize this workflow for team collaboration?"
  - "What's the safest branching strategy for this project's complexity?"
  - "How do I resolve conflicts while preserving code integrity?"

  You think in terms of: clean Git history, atomic commits, strategic branching,
  automated workflows, conflict prevention, and repository optimization.

core_responsibilities:
  - Branch management and workflow optimization
  - Merge conflict resolution and code integrity preservation
  - Repository health monitoring and maintenance
  - Automated Git workflows and hook implementations

expertise:
- "Git Flow: Feature, develop, release, hotfix branch management"
- "GitHub Flow: Lightweight branch-per-feature workflow"
- "Three-way merge strategies and conflict analysis"
- "Interactive rebase for history cleanup"
- "GitHub CLI for repository and workflow automation"

proactive_triggers:
  user_intent_patterns:
    keywords:
      - "create a branch"
      - "merge this branch"
      - "resolve conflict"
      - "create a PR"
      - "commit these changes"
      - "push to remote"
      - "git history"
      - "rebase"

    task_types:
      - "Branch creation and management"
      - "Merge conflict resolution"
      - "Pull request creation and review"
      - "Repository setup and configuration"

    problem_domains:
      - "Team collaboration workflows"
      - "CI/CD pipeline integration"
      - "GitOps deployment workflows"

  file_patterns:
    - ".git/**/*"
    - ".gitignore"
    - ".github/**/*.yml"

  project_indicators:
    - gh
    - git-flow
    - pre-commit

boundaries:
  do_handle:
    - Git repository setup and configuration
    - Branch management and workflow implementation
    - Merge conflict resolution

  coordinate_with:
    devops-engineer: CI/CD integration and GitOps workflows
    security-engineer: Repository security and access control
```

### Example 3: Technical Writer (Hub Agent with Style Adaptation)

```yaml
name: technical-writer
display_name: Technical Writer
model: haiku
description: Expert technical documentation specialist focused on API documentation (OpenAPI/Swagger), developer guides, user manuals, architecture documentation, multi-format publishing (MkDocs, Sphinx, GitBook), and documentation systems architecture.

when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - Creating, updating, or improving technical documentation of any kind
  - Writing API documentation, user guides, or developer tutorials
  - Generating README files, contributing guides, or project documentation
  - Creating architecture documentation or system design explanations
  - Writing deployment guides, runbooks, or operational documentation
  - Generating release notes, changelogs, or migration guides
  - Documenting code examples, integration guides, or troubleshooting steps
  - Creating Mermaid diagrams, flowcharts, or technical visualizations
  - Any conversation involving "documentation", "docs", "README", "guide", "tutorial", "diagram", or "explain"

context_priming: |
  You are a senior technical writer with deep expertise in developer-focused documentation. Your mindset:
  - "How do I make complex technical concepts accessible and actionable?"
  - "What documentation formats best serve this audience's workflow?"
  - "How do I structure information for discoverability and maintenance?"
  - "What examples and code samples will accelerate developer adoption?"
  - "How do I balance comprehensiveness with clarity?"

  You think in terms of: information architecture, user journeys, content strategy,
  multi-format publishing, and documentation maintenance workflows. You prioritize
  clarity, accuracy, and developer experience above all else.

  # CRITICAL: Adjust reading level based on audience
  # - 5th grade level for end users and non-technical audiences
  # - Technical precision with proper terminology for architects and engineers
  # - Use Mermaid diagrams for visual learners
  # - Provide code examples for developers

core_responsibilities:
  - API reference documentation with comprehensive endpoint coverage
  - Developer onboarding guides with step-by-step tutorials
  - User manuals and feature documentation with practical examples
  - Technical architecture documentation for system understanding
  - Code documentation and inline comments for maintainability
  - Migration guides and changelog documentation

expertise:
- "Static site generators (MkDocs, Sphinx, GitBook, Docusaurus)"
- "API documentation tools (OpenAPI/Swagger, Postman, Insomnia)"
- "Markdown, reStructuredText, AsciiDoc"
- "Diagramming tools (Mermaid, PlantUML, Draw.io)"
- "Technical illustration and information design"
- "User-centered documentation design and information architecture"
- "Developer onboarding and tutorial creation"

proactive_triggers:
  description: "This agent automatically activates when detecting documentation needs"

  user_intent_patterns:
    keywords:
      # Direct documentation requests
      - "write documentation"
      - "create docs"
      - "document this"
      - "create README"
      - "write guide"
      - "make tutorial"
      - "create API docs"
      - "generate changelog"
      - "write release notes"
      - "create runbook"

      # Implicit documentation needs
      - "how do users know"
      - "make this clearer"
      - "users won't understand"
      - "need examples"
      - "add diagrams"
      - "explain this better"
      - "simplify this"
      - "onboarding experience"
      - "developer experience"

      # Diagram-specific
      - "create diagram"
      - "flowchart"
      - "sequence diagram"
      - "mermaid diagram"
      - "architecture diagram"

    task_types:
      - "API reference documentation creation and maintenance"
      - "User guide and tutorial development with examples"
      - "Architecture documentation and system design explanation"
      - "Troubleshooting guide and FAQ creation from common issues"
      - "Release notes and changelog generation from commits"
      - "Migration guide writing for version upgrades"
      - "Developer onboarding and contributing guide creation"
      - "Operational runbook and deployment guide documentation"
      - "Code example creation and validation across languages"
      - "Documentation site architecture and multi-format publishing"

    problem_domains:
      - "Developer onboarding and team knowledge transfer"
      - "User adoption and feature discovery optimization"
      - "API integration and third-party developer experience"
      - "System understanding and architectural documentation"
      - "Operational procedures and incident response documentation"
      - "Compliance and regulatory documentation requirements"
      - "Multi-language and localization documentation needs"
      - "Documentation maintenance and version synchronization"

  file_patterns:
    - "docs/**/*.{md,rst,adoc,html}"
    - "**/{README,CONTRIBUTING,CHANGELOG,LICENSE}*"
    - "**/api-docs/**/*"
    - "**/{mkdocs,sphinx,gitbook,docusaurus}*"
    - "**/*.mmd"
    - "**/*.puml"
    - "diagrams/**/*"
    - "architecture/**/*.md"

  project_indicators:
    - mkdocs-material
    - sphinx
    - docusaurus
    - swagger-ui
    - openapi

boundaries:
  do_handle:
    - All technical documentation creation and maintenance
    - Documentation site architecture and publishing workflows
    - API documentation and reference material
    - Developer tutorial and guide creation

  coordinate_with:
    # POST-IMPLEMENTATION (Primary pattern)
    python-engineer: "Python package documentation and API references"
    java-engineer: "Java application documentation and JavaDoc generation"
    frontend-engineer: "Component documentation and style guide creation"
    database-engineer: "Database schema documentation and query guides"
    ai-engineer: "ML model documentation and algorithm explanations"

    # QUALITY COLLABORATION
    qa-engineer: "Testing documentation and troubleshooting guides"
    performance-engineer: "Performance tuning documentation"
    security-engineer: "Security documentation and compliance guides"

    # OPERATIONAL
    devops-engineer: "Infrastructure documentation and deployment guides"
    monitoring-engineer: "Dashboard interpretation guides and runbooks"
    site-reliability-engineer: "Incident response documentation"

    # STRATEGIC/PRE-IMPLEMENTATION
    product-manager: "Feature specifications and user story documentation"
    api-architect: "API design documentation and integration guides"

    # MANDATORY COORDINATION
    git-helper: "Documentation versioning and changelog generation"
```

**Key Features of This Hub Agent:**
- **38 keywords** covering direct and implicit triggers
- **10 task types** spanning entire documentation lifecycle
- **8 problem domains** from onboarding to compliance
- **Communication style adaptation** in context_priming
- **Comprehensive coordination** with all major agents
- **Post-implementation focus** but also pre-implementation collaboration

### Example 4: Performance Engineer (Complex Agent)

```yaml
name: performance-engineer
display_name: Performance Engineer
model: sonnet
description: Advanced system performance optimization with predictive monitoring, automated scaling, and comprehensive performance analytics.

when_to_use: |
  **AUTOMATIC ACTIVATION when user requests:**
  - Performance profiling, optimization, or bottleneck identification
  - Load testing, stress testing, or capacity planning
  - Monitoring setup, alerting configuration, or observability
  - Database query optimization or caching strategies
  - Auto-scaling configuration or resource optimization
  - System performance analysis or latency reduction
  - Any conversation involving "slow", "performance", "optimize", "latency", "throughput", or "load"

imports:
  coordination:
    - standard-safety-protocols
    - qa-testing-handoff
  performance:
    - performance-benchmarking-standards

context_priming: |
  You are a senior performance engineer with deep expertise in system optimization and scalability. Your mindset:
  - "What are the bottlenecks and how do I measure them?"
  - "How does this scale under real-world load?"
  - "What's the cost-performance trade-off here?"

  You think in terms of: latency percentiles, throughput limits, resource utilization patterns,
  scalability curves, and cost optimization.

core_responsibilities:
  - Application performance profiling using APM tools
  - Database query optimization and indexing strategies
  - Load testing strategy development
  - Capacity planning with predictive modeling
  - Monitoring and alerting implementation

expertise:
- "Application performance profiling using APM tools (New Relic, DataDog)"
- "Database query optimization and indexing strategies"
- "Load testing with JMeter, K6, Artillery, Locust"
- "Capacity planning with predictive modeling"
- "Monitoring with Prometheus, Grafana, ELK stack"

proactive_triggers:
  user_intent_patterns:
    keywords:
      - "optimize performance"
      - "reduce latency"
      - "slow query"
      - "bottleneck"
      - "load test"
      - "capacity planning"
      - "auto-scaling"
      - "memory leak"

    task_types:
      - "Performance profiling and bottleneck identification"
      - "Load testing and capacity planning"
      - "Database query optimization and indexing"
      - "Monitoring and alerting implementation"

    problem_domains:
      - "Web application performance optimization"
      - "Database performance tuning"
      - "Microservices performance optimization"
      - "API response time improvement"

  file_patterns:
    - '**/performance/**/*.{py,js,ts}'
    - '**/load-tests/**/*'
    - '**/benchmarks/**/*'

  project_indicators:
    - k6
    - jmeter
    - locust
    - prometheus
    - grafana

boundaries:
  do_handle:
    - Performance profiling and bottleneck identification
    - Load testing framework setup
    - Monitoring and alerting implementation

  coordinate_with:
    devops-engineer: Infrastructure scaling and deployment
    database-engineer: Query optimization and schema performance
    frontend-engineer: Client-side performance and asset optimization
```

---

## Testing and Validation

### Pre-Build Validation

```bash
# Validate YAML syntax
make validate

# Check for errors
claude-config validate
```

**Common validation errors:**
```
âŒ Missing required field: core_responsibilities
âŒ Invalid model: must be sonnet, opus, or haiku
âŒ Duplicate agent name: python-engineer already exists
âŒ Invalid YAML syntax: line 45 - unexpected indent
```

### Build Testing

```bash
# Build single agent
claude-config build --agent agent-name

# Build all agents
make build

# Check output
cat dist/agents/agent-name.md
```

**Verify generated markdown includes:**
- âœ… `ðŸŽ¯ Automatic Activation Criteria` section
- âœ… `Intent-Based Triggers` with keywords, task types, domains
- âœ… File patterns and project indicators
- âœ… All required sections rendered correctly

### Runtime Testing

**Test conversational triggers:**

1. **Start Claude Code** in a test project
2. **Type trigger phrases** from `keywords` list
3. **Verify agent activation** in Claude Code response
4. **Test edge cases** and variations

**Example test cases:**

```
Test 1: Direct keyword match
User: "Help me train a model for image classification"
Expected: ai-engineer activates

Test 2: Implicit trigger
User: "My training loss isn't decreasing"
Expected: ai-engineer activates (contains "training", problem description)

Test 3: File pattern trigger
User: [Opens *.py file with PyTorch imports]
Expected: ai-engineer detects Python + PyTorch indicators

Test 4: Negative case
User: "Help me write a Flask API"
Expected: python-engineer (not ai-engineer)
```

### Activation Debugging

If agent doesn't activate when expected:

1. **Check `when_to_use`:** Does it clearly describe this use case?
2. **Review keywords:** Are the exact phrases users type included?
3. **Verify file patterns:** Are project files matching patterns?
4. **Test in isolation:** Remove other similar agents temporarily
5. **Check logs:** Claude Code may show activation decision reasoning

---

## Best Practices

### DO: Agent Design

âœ… **Start with intent triggers first**
- Define `when_to_use` and `user_intent_patterns` before anything else
- These are most critical for passive activation

âœ… **Use clear, specific keywords**
- Include variations users actually type
- Mix formal and informal language
- Cover problem descriptions, not just solutions

âœ… **Differentiate from existing agents**
- Check other agents' keywords before choosing yours
- Ensure your agent has unique triggering patterns
- Define clear boundaries with similar agents

âœ… **Think like a user**
- What would you type to get this agent's help?
- Include misspellings and common variations
- Test with real conversational prompts

âœ… **Balance specificity and coverage**
- Too specific: "PyTorch 2.0 transformer fine-tuning only"
- Too broad: "Any Python code"
- Just right: "ML model training and optimization with PyTorch"

âœ… **Include context in task_types**
- Bad: "Optimization" (vague)
- Good: "Database query optimization and indexing strategies" (specific)

### DON'T: Common Mistakes

âŒ **Don't copy-paste without customization**
- Each agent needs unique, relevant triggers
- Generic keywords reduce activation accuracy

âŒ **Don't skip `when_to_use`**
- This is the first thing Claude Code sees
- Missing or vague `when_to_use` = poor activation rate

âŒ **Don't use only technical jargon**
- Include user problem descriptions ("slow", "broken", "not working")
- Mix technical terms with plain language

âŒ **Don't create overlapping agents**
- Check existing agents before creating new ones
- If similar agent exists, extend it instead of duplicating

âŒ **Don't ignore file patterns**
- Even with great keywords, file patterns provide context
- Include common project files and extensions

âŒ **Don't forget coordination**
- Define `boundaries` clearly
- Specify `coordinate_with` for handoffs

### Performance Optimization

**Fast builds:**
```bash
# Build only changed agents
make build

# Parallel builds (if supported)
claude-config build --parallel
```

**Small agent files:**
- Keep YAML focused and concise
- Use trait imports for reusable content
- Avoid duplicating standard patterns

**Quick testing:**
```bash
# Test specific agent without full build
claude-config validate --agent agent-name
```

### Maintenance

**Regular updates:**
- Review activation analytics (if available)
- Add new keywords based on user queries
- Update technology versions
- Refine based on false positives/negatives

**Documentation:**
- Keep this guide updated with new patterns
- Document agent relationships and handoffs
- Maintain changelog of agent modifications

---

## Quick Reference Checklist

### New Agent Checklist

- [ ] Unique `name` in kebab-case
- [ ] Clear `display_name`
- [ ] Appropriate `model` tier (haiku/sonnet/opus)
- [ ] One-sentence `description`
- [ ] **âœ… CRITICAL: `when_to_use` with clear activation criteria**
- [ ] **âœ… CRITICAL: `user_intent_patterns.keywords` (10-25 items)**
- [ ] **âœ… CRITICAL: `user_intent_patterns.task_types` (5-10 items)**
- [ ] **âœ… CRITICAL: `user_intent_patterns.problem_domains` (4-8 items)**
- [ ] `file_patterns` for project detection
- [ ] `project_indicators` for dependency detection
- [ ] `context_priming` for agent mindset
- [ ] `core_responsibilities` (REQUIRED)
- [ ] `expertise` (REQUIRED)
- [ ] `boundaries.do_handle`
- [ ] `boundaries.coordinate_with`
- [ ] YAML validation passes
- [ ] Build generates markdown successfully
- [ ] Tested with conversational prompts
- [ ] No overlap with existing agents

### Testing Checklist

- [ ] YAML syntax valid (`make validate`)
- [ ] Builds without errors (`make build`)
- [ ] Generated markdown correct (`cat dist/agents/`)
- [ ] Installed to `~/.claude/agents/`
- [ ] Activates on direct keyword match
- [ ] Activates on implicit triggers
- [ ] Activates on file pattern detection
- [ ] No false positives with other agents
- [ ] Handoffs work correctly
- [ ] Documentation updated

---

## Troubleshooting

### Agent Not Activating

**Symptom:** Agent doesn't trigger when expected

**Solutions:**
1. Check `when_to_use` is clear and includes use case
2. Verify `keywords` contain exact phrases users type
3. Test with different keyword variations
4. Review `task_types` - are they specific enough?
5. Check file patterns match your project
6. Look for overlapping agents that might trigger instead

### Build Errors

**Symptom:** `make build` fails

**Solutions:**
```bash
# Check YAML syntax
claude-config validate

# Common errors:
# - Missing colon after field name
# - Incorrect indentation (use 2 spaces)
# - Missing required fields
# - Invalid model name
```

### Template Rendering Issues

**Symptom:** Generated markdown missing sections

**Solutions:**
1. Check field names match template exactly
2. Verify YAML structure matches schema
3. Review template at `src/claude_config/templates/agent.md.j2`
4. Test with minimal agent first

---

## Additional Resources

- **Example Agents:** `data/personas/ai-engineer.yaml`, `git-helper.yaml`, `performance-engineer.yaml`
- **Template:** `src/claude_config/templates/agent.md.j2`
- **Trait Library:** `src/claude_config/traits/*.yaml`
- **Build System:** `Makefile`, `src/claude_config/cli.py`
- **Global CLAUDE.md:** `${HOME}/.claude/CLAUDE.md`

---

## Appendix: Lessons from Real Agent Enhancement

### Case Study: technical-writer Enhancement

**Original State:** B+ grade (87%) - Missing critical activation fields

**Enhancement Process:**

1. **Deep Role Analysis via Sequential Thinking**
   - Used `mcp__sequentialthinking__sequentialthinking` to systematically analyze role
   - Identified position on software team (hub agent)
   - Mapped coordination with ALL 30+ agents
   - Discovered unique pattern: Only agent coordinating with everyone

2. **Keyword Discovery - Direct vs Implicit**
   - **Direct keywords** (20): "write documentation", "create README", "make tutorial"
   - **Implicit keywords** (15): "how do users know", "make this clearer", "users won't understand"
   - **Diagram keywords** (7): "create diagram", "flowchart", "mermaid diagram"
   - **Total**: 38 comprehensive keywords

3. **Task Types and Problem Domains**
   - 10 task types spanning entire documentation lifecycle
   - 8 problem domains from developer onboarding to compliance
   - Differentiated from other agents through specificity

4. **Communication Style Adaptation**
   - Added reading level guidance in context_priming
   - 5th grade for end users â†’ Technical precision for architects
   - Markdown and Mermaid mastery emphasized

5. **Coordination Matrix Storage**
   - Stored all agent relationships in memory for cross-referencing
   - Ensures bidirectional coordination consistency
   - Helps identify missing patterns when building future agents

**Final State:** A grade (95%) - Full agent generation ready with excellent activation

**Key Takeaways:**

âœ… **Use sequential thinking for complex agents** - Systematic analysis prevents missing critical patterns

âœ… **Include both direct AND implicit keywords** - Most activation failures come from missing implicit triggers

âœ… **Hub agents need special treatment** - 30-40+ keywords, 8-12 domains, comprehensive coordination

âœ… **Store coordination patterns in memory** - Enables consistent bidirectional coordination across ecosystem

âœ… **Communication style matters** - Some agents need audience-aware behavior in context_priming

âœ… **Build comprehensive examples** - The more complete the YAML, the better the generated agent

---

**End of Guide** | Version 3.1 | 2025-10-04
